{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esteb\\Desktop\\Python\\Technical_Analysis_Proyect_ML\\Technical_Analysis_Proyect\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ta\n",
    "import optuna\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random=1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Rerris/Technical_Analysis_Proyect/main/data/aapl_1d_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    response = requests.get(url, verify=True)\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    data=data[\"Close\"]\n",
    "    data=pd.DataFrame(data)\n",
    "    data[\"Closet-1\"]=data[\"Close\"].shift(-1)\n",
    "    data[\"Closet-2\"]=data[\"Close\"].shift(-2)\n",
    "    rsi_indicator = ta.momentum.RSIIndicator(close=data['Close'], window=14)\n",
    "    data['RSI'] = rsi_indicator.rsi()\n",
    "    #MACD\n",
    "    macd_indicator = ta.trend.MACD(close=data['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "    data['macd'] = macd_indicator.macd()\n",
    "    data['macd_signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "    # Crear instancia del indicador EMA con períodos de 13 y 48\n",
    "    ema_13_indicator = ta.trend.EMAIndicator(close=data['Close'], window=13)\n",
    "    ema_48_indicator = ta.trend.EMAIndicator(close=data['Close'], window=48)\n",
    "        \n",
    "    # Calcular las EMAs 13 y 48 \n",
    "    data['Ema 13'] = ema_13_indicator.ema_indicator()\n",
    "    data['Ema 48'] = ema_48_indicator.ema_indicator()\n",
    "    # Inicializar el indicador EMA con una ventana de 200 períodos\n",
    "    ema_indicator = ta.trend.EMAIndicator(close=data['Close'], window=200)\n",
    "    # Calcular la EMA 200\n",
    "\n",
    "    data['Ema 200'] = ema_indicator.ema_indicator()\n",
    "    data[\"Long\"] = False\n",
    "    data[\"Short\"] = False\n",
    "\n",
    "    for i in range(len(data[\"Close\"])):\n",
    "\n",
    "        if i + 10 < len(data[\"Close\"]):\n",
    "            if data[\"Close\"][i] > data[\"Close\"][i + 10]:\n",
    "                data.loc[i, \"Long\"] = True\n",
    "            elif data[\"Close\"][i] < data[\"Close\"][i + 10]:\n",
    "                data.loc[i, \"Short\"] = True\n",
    "    data=data\n",
    "    return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Closet-1</th>\n",
       "      <th>Closet-2</th>\n",
       "      <th>RSI</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>Ema 13</th>\n",
       "      <th>Ema 48</th>\n",
       "      <th>Ema 200</th>\n",
       "      <th>Long</th>\n",
       "      <th>Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129.41</td>\n",
       "      <td>131.01</td>\n",
       "      <td>126.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131.01</td>\n",
       "      <td>126.60</td>\n",
       "      <td>130.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126.60</td>\n",
       "      <td>130.92</td>\n",
       "      <td>132.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130.92</td>\n",
       "      <td>132.05</td>\n",
       "      <td>128.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132.05</td>\n",
       "      <td>128.98</td>\n",
       "      <td>128.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>193.60</td>\n",
       "      <td>193.05</td>\n",
       "      <td>193.15</td>\n",
       "      <td>54.609704</td>\n",
       "      <td>2.636316</td>\n",
       "      <td>3.236689</td>\n",
       "      <td>194.600511</td>\n",
       "      <td>188.472310</td>\n",
       "      <td>177.514883</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>193.05</td>\n",
       "      <td>193.15</td>\n",
       "      <td>193.58</td>\n",
       "      <td>53.026881</td>\n",
       "      <td>2.306308</td>\n",
       "      <td>3.050613</td>\n",
       "      <td>194.379009</td>\n",
       "      <td>188.659155</td>\n",
       "      <td>177.669462</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>193.15</td>\n",
       "      <td>193.58</td>\n",
       "      <td>192.53</td>\n",
       "      <td>53.291960</td>\n",
       "      <td>2.029448</td>\n",
       "      <td>2.846380</td>\n",
       "      <td>194.203437</td>\n",
       "      <td>188.842454</td>\n",
       "      <td>177.823497</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>193.58</td>\n",
       "      <td>192.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.481471</td>\n",
       "      <td>1.823710</td>\n",
       "      <td>2.641846</td>\n",
       "      <td>194.114374</td>\n",
       "      <td>189.035824</td>\n",
       "      <td>177.980278</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>192.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.061845</td>\n",
       "      <td>1.557975</td>\n",
       "      <td>2.425072</td>\n",
       "      <td>193.888035</td>\n",
       "      <td>189.178443</td>\n",
       "      <td>178.125051</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Close  Closet-1  Closet-2        RSI      macd  macd_signal      Ema 13  \\\n",
       "0    129.41    131.01    126.60        NaN       NaN          NaN         NaN   \n",
       "1    131.01    126.60    130.92        NaN       NaN          NaN         NaN   \n",
       "2    126.60    130.92    132.05        NaN       NaN          NaN         NaN   \n",
       "3    130.92    132.05    128.98        NaN       NaN          NaN         NaN   \n",
       "4    132.05    128.98    128.80        NaN       NaN          NaN         NaN   \n",
       "..      ...       ...       ...        ...       ...          ...         ...   \n",
       "748  193.60    193.05    193.15  54.609704  2.636316     3.236689  194.600511   \n",
       "749  193.05    193.15    193.58  53.026881  2.306308     3.050613  194.379009   \n",
       "750  193.15    193.58    192.53  53.291960  2.029448     2.846380  194.203437   \n",
       "751  193.58    192.53       NaN  54.481471  1.823710     2.641846  194.114374   \n",
       "752  192.53       NaN       NaN  51.061845  1.557975     2.425072  193.888035   \n",
       "\n",
       "         Ema 48     Ema 200   Long  Short  \n",
       "0           NaN         NaN   True  False  \n",
       "1           NaN         NaN  False   True  \n",
       "2           NaN         NaN  False   True  \n",
       "3           NaN         NaN  False   True  \n",
       "4           NaN         NaN  False   True  \n",
       "..          ...         ...    ...    ...  \n",
       "748  188.472310  177.514883  False  False  \n",
       "749  188.659155  177.669462  False  False  \n",
       "750  188.842454  177.823497  False  False  \n",
       "751  189.035824  177.980278  False  False  \n",
       "752  189.178443  178.125051  False  False  \n",
       "\n",
       "[753 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_data(url).iloc[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_long(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Long']\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo de regresión logística\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Imprimir el informe de clasificación\n",
    "    #print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlogistic_regression_long\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mlogistic_regression_long\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLong\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjuntos de entrenamiento y prueba\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Inicializar el modelo de regresión logística\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n",
      "File \u001b[1;32mc:\\Users\\esteb\\Desktop\\Python\\Technical_Analysis_Proyect_ML\\Technical_Analysis_Proyect\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\esteb\\Desktop\\Python\\Technical_Analysis_Proyect_ML\\Technical_Analysis_Proyect\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\esteb\\Desktop\\Python\\Technical_Analysis_Proyect_ML\\Technical_Analysis_Proyect\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "logistic_regression_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_short(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Short']\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo de regresión logística\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Imprimir el informe de clasificación\n",
    "    #print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_short(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine_long(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Long']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el clasificador SVM\n",
    "    clf = svm.SVC(kernel='linear')  # Puedes cambiar el kernel según tus necesidades (lineal, polinomial, RBF, etc.)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine_short(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Short']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el clasificador SVM\n",
    "    clf = svm.SVC(kernel='linear')  # Puedes cambiar el kernel según tus necesidades (lineal, polinomial, RBF, etc.)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine_short(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_long(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Long']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convertir los datos a un formato específico para XGBoost (DMatrix)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Definir los parámetros del modelo\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',  # Problema de clasificación binaria\n",
    "        'eval_metric': 'logloss',  # Métrica de evaluación\n",
    "        'eta': 0.1,  # Tasa de aprendizaje\n",
    "        'max_depth': 6,  # Profundidad máxima del árbol\n",
    "        'subsample': 0.8,  # Proporción de muestras utilizadas para entrenar cada árbol\n",
    "        'colsample_bytree': 0.8  # Proporción de características utilizadas para entrenar cada árbol\n",
    "    }\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    num_round = 100  # Número de iteraciones de entrenamiento (número de árboles)\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = [1 if pred > 0.5 else 0 for pred in y_pred_proba]  # Convertir probabilidades en etiquetas binarias\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_short(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Short']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convertir los datos a un formato específico para XGBoost (DMatrix)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Definir los parámetros del modelo\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',  # Problema de clasificación binaria\n",
    "        'eval_metric': 'logloss',  # Métrica de evaluación\n",
    "        'eta': 0.1,  # Tasa de aprendizaje\n",
    "        'max_depth': 6,  # Profundidad máxima del árbol\n",
    "        'subsample': 0.8,  # Proporción de muestras utilizadas para entrenar cada árbol\n",
    "        'colsample_bytree': 0.8  # Proporción de características utilizadas para entrenar cada árbol\n",
    "    }\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    num_round = 100  # Número de iteraciones de entrenamiento (número de árboles)\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = [1 if pred > 0.5 else 0 for pred in y_pred_proba]  # Convertir probabilidades en etiquetas binarias\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_short(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizing using \"Long\" as the target variable\n",
    "X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "y = data['Long']\n",
    "\n",
    "# Optimizing Logistic Regression\n",
    "def optimize_logistic_regression(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000) # Maximum number of iterations\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=max_iter)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing SVM\n",
    "def optimize_svm(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly']) # Kernel type\n",
    "    \n",
    "    model = svm.SVC(C=C, kernel=kernel)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing XGBoost\n",
    "def optimize_xgboost(trial):\n",
    "    eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10) # Maximum depth of the tree\n",
    "    subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
    "    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
    "    \n",
    "    model = xgb.XGBClassifier(eta=eta, max_depth=max_depth, subsample=subsample, colsample_bytree=colsample_bytree)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Ojective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['logistic_regression', 'svm', 'xgboost'])\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        return optimize_logistic_regression(trial)\n",
    "    elif model_name == 'svm':\n",
    "        return optimize_svm(trial)\n",
    "    elif model_name == 'xgboost':\n",
    "        return optimize_xgboost(trial)\n",
    "\n",
    "# Optimize hyperparameters using Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100) # We decided to do 100 trials becasue we noticed that the the study convverges\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Print optimal hyperparameters\n",
    "for param, value in best_params.items():\n",
    "    print(f\"Optimal {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing using \"Short\" as the target variable\n",
    "X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "y = data['Short']\n",
    "\n",
    "# Optimizing Logistic Regression\n",
    "def optimize_logistic_regression(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000) # Maximum number of iterations\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=max_iter)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing SVM\n",
    "def optimize_svm(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly']) # Kernel type\n",
    "    \n",
    "    model = svm.SVC(C=C, kernel=kernel)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing XGBoost\n",
    "def optimize_xgboost(trial):\n",
    "    eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10) # Maximum depth of the tree\n",
    "    subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
    "    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
    "    \n",
    "    model = xgb.XGBClassifier(eta=eta, max_depth=max_depth, subsample=subsample, colsample_bytree=colsample_bytree)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Ojective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['logistic_regression', 'svm', 'xgboost'])\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        return optimize_logistic_regression(trial)\n",
    "    elif model_name == 'svm':\n",
    "        return optimize_svm(trial)\n",
    "    elif model_name == 'xgboost':\n",
    "        return optimize_xgboost(trial)\n",
    "\n",
    "# Optimize hyperparameters using Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100) # We decided to do 100 trials becasue we noticed that the the study converges\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Print optimal hyperparameters\n",
    "for param, value in best_params.items():\n",
    "    print(f\"Optimal {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_logistic_regression(data, model, target, take_profit, stop_loss, initial_capital):\n",
    "    # Crear una copia de los datos\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Crear una nueva columna con las predicciones del modelo\n",
    "    data['Prediction'] = model.predict(data[['RSI', 'Ema 13', 'Ema 200']])\n",
    "    \n",
    "    # Calcular el rendimiento diario\n",
    "    data['Return'] = data['Closet-1'] / data['Close'] - 1\n",
    "    \n",
    "    # Calcular el rendimiento diario de la estrategia\n",
    "    data['Strategy Return'] = data['Return'] * data['Prediction']\n",
    "    \n",
    "    # Calcular el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'] = (data['Strategy Return'] + 1).cumprod()\n",
    "    \n",
    "    # Inicializar el capital\n",
    "    capital = initial_capital\n",
    "    \n",
    "    # Inicializar el estado de la operación\n",
    "    in_trade = False\n",
    "    \n",
    "    # Inicializar el precio de entrada\n",
    "    entry_price = 0\n",
    "    \n",
    "    # Recorrer los datos\n",
    "    for i in range(len(data)):\n",
    "        # Si estamos en una operación\n",
    "        if in_trade:\n",
    "            # Si alcanzamos el take profit o el stop loss\n",
    "            if data['Close'].iloc[i] >= entry_price * (1 + take_profit) or data['Close'].iloc[i] <= entry_price * (1 - stop_loss):\n",
    "                # Salir de la operación\n",
    "                in_trade = False\n",
    "        # Si no estamos en una operación y el modelo predice una entrada\n",
    "        elif data['Prediction'].iloc[i] == 1:\n",
    "            # Entrar en la operación\n",
    "            in_trade = True\n",
    "            entry_price = data['Close'].iloc[i]\n",
    "            # Use only 10% of the capital for each trade\n",
    "            trade_amount = capital * 0.00001\n",
    "            capital -= trade_amount  # Descontar el precio de entrada del capital\n",
    "    \n",
    "    # Graficar el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'].plot(figsize=(10, 6))\n",
    "    plt.title(f'Cumulative Strategy Return ({target})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(data[target], data['Prediction'])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Imprimir el informe de clasificación\n",
    "    print(classification_report(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir la matriz de confusión\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir el capital final\n",
    "    print(\"Final capital:\", capital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo de regresión logística para \"Long\"\n",
    "model = LogisticRegression(C=0.01, max_iter=100)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Long'])\n",
    "backtest_logistic_regression(data, model, 'Long', 0.05, 0.02, 1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo de regresión logística para \"Short\"\n",
    "model = LogisticRegression(C=0.01, max_iter=100)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Short'])\n",
    "backtest_logistic_regression(data, model, 'Short', 0.05, 0.02, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_svm(data, model, target, take_profit, stop_loss, initial_capital):\n",
    "    # Crear una copia de los datos\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Crear una nueva columna con las predicciones del modelo\n",
    "    data['Prediction'] = model.predict(data[['RSI', 'Ema 13', 'Ema 200']])\n",
    "    \n",
    "    # Calcular el rendimiento diario\n",
    "    data['Return'] = data['Closet-1'] / data['Close'] - 1\n",
    "    \n",
    "    # Calcular el rendimiento diario de la estrategia\n",
    "    data['Strategy Return'] = data['Return'] * data['Prediction']\n",
    "    \n",
    "    # Calcular el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'] = (data['Strategy Return'] + 1).cumprod()\n",
    "    \n",
    "    # Inicializar el capital\n",
    "    capital = initial_capital\n",
    "    \n",
    "    # Inicializar el estado de la operación\n",
    "    in_trade = False\n",
    "    \n",
    "    # Inicializar el precio de entrada\n",
    "    entry_price = 0\n",
    "    \n",
    "    # Recorrer los datos\n",
    "    for i in range(len(data)):\n",
    "        # Si estamos en una operación\n",
    "        if in_trade:\n",
    "            # Si alcanzamos el take profit o el stop loss\n",
    "            if data['Close'].iloc[i] >= entry_price * (1 + take_profit) or data['Close'].iloc[i] <= entry_price * (1 - stop_loss):\n",
    "                # Salir de la operación\n",
    "                in_trade = False\n",
    "        # Si no estamos en una operación y el modelo predice una entrada\n",
    "        elif data['Prediction'].iloc[i] == 1:\n",
    "            # Entrar en la operación\n",
    "            in_trade = True\n",
    "            entry_price = data['Close'].iloc[i]\n",
    "            # Use only 10% of the capital for each trade\n",
    "            trade_amount = capital * 0.00001\n",
    "            capital -= trade_amount  # Descontar el precio de entrada del capital\n",
    "    \n",
    "    # Graficar el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'].plot(figsize=(10, 6))\n",
    "    plt.title(f'Cumulative Strategy Return ({target})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(data[target], data['Prediction'])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Imprimir el informe de clasificación\n",
    "    print(classification_report(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir la matriz de confusión\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir el capital final\n",
    "    print(\"Final capital:\", capital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo SVM para \"Long\"\n",
    "model = svm.SVC(C=0.01, kernel='rbf')\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Long'])\n",
    "backtest_svm(data, model, 'Long', 0.05, 0.02, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo SVM para \"Short\"\n",
    "model = svm.SVC(C=0.01, kernel='rbf')\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Short'])\n",
    "backtest_svm(data, model, 'Short', 0.05, 0.02, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_xgboost(data, model, target, take_profit, stop_loss, initial_capital):\n",
    "    # Crear una copia de los datos\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Crear una nueva columna con las predicciones del modelo\n",
    "    data['Prediction'] = model.predict(data[['RSI', 'Ema 13', 'Ema 200']])\n",
    "    \n",
    "    # Calcular el rendimiento diario\n",
    "    data['Return'] = data['Closet-1'] / data['Close'] - 1\n",
    "    \n",
    "    # Calcular el rendimiento diario de la estrategia\n",
    "    data['Strategy Return'] = data['Return'] * data['Prediction']\n",
    "    \n",
    "    # Calcular el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'] = (data['Strategy Return'] + 1).cumprod()\n",
    "    \n",
    "    # Inicializar el capital\n",
    "    capital = initial_capital\n",
    "    \n",
    "    # Inicializar el estado de la operación\n",
    "    in_trade = False\n",
    "    \n",
    "    # Inicializar el precio de entrada\n",
    "    entry_price = 0\n",
    "    \n",
    "    # Recorrer los datos\n",
    "    for i in range(len(data)):\n",
    "        # Si estamos en una operación\n",
    "        if in_trade:\n",
    "            # Si alcanzamos el take profit o el stop loss\n",
    "            if data['Close'].iloc[i] >= entry_price * (1 + take_profit) or data['Close'].iloc[i] <= entry_price * (1 - stop_loss):\n",
    "                # Salir de la operación\n",
    "                in_trade = False\n",
    "        # Si no estamos en una operación y el modelo predice una entrada\n",
    "        elif data['Prediction'].iloc[i] == 1:\n",
    "            # Entrar en la operación\n",
    "            in_trade = True\n",
    "            entry_price = data['Close'].iloc[i]\n",
    "            # Use only 10% of the capital for each trade\n",
    "            trade_amount = capital * 0.000010\n",
    "            capital -= trade_amount  # Descontar el precio de entrada del capital\n",
    "    \n",
    "    # Graficar el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'].plot(figsize=(10, 6))\n",
    "    plt.title(f'Cumulative Strategy Return ({target})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(data[target], data['Prediction'])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Imprimir el informe de clasificación\n",
    "    print(classification_report(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir la matriz de confusión\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir el capital final\n",
    "    print(\"Final capital:\", capital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo XGBoost para \"Long\"\n",
    "model = xgb.XGBClassifier(eta=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Long'])\n",
    "backtest_xgboost(data, model, 'Long', 0.05, 0.02, 1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo XGBoost para \"Short\"\n",
    "model = xgb.XGBClassifier(eta=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Short'])\n",
    "backtest_xgboost(data, model, 'Short', 0.05, 0.02, 1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
