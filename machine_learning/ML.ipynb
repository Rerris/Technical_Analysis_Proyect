{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ta\n",
    "import optuna\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random=1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Rerris/Technical_Analysis_Proyect/main/data/aapl_5m_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    response = requests.get(url, verify=True)\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    data=data[\"Close\"]\n",
    "    data=pd.DataFrame(data)\n",
    "    data[\"Closet-1\"]=data[\"Close\"].shift(-1)\n",
    "    data[\"Closet-2\"]=data[\"Close\"].shift(-2)\n",
    "    rsi_indicator = ta.momentum.RSIIndicator(close=data['Close'], window=14)\n",
    "    data['RSI'] = rsi_indicator.rsi()\n",
    "    #MACD\n",
    "    macd_indicator = ta.trend.MACD(close=data['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "    data['macd'] = macd_indicator.macd()\n",
    "    data['macd_signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "    # Crear instancia del indicador EMA con períodos de 13 y 48\n",
    "    ema_13_indicator = ta.trend.EMAIndicator(close=data['Close'], window=13)\n",
    "    ema_48_indicator = ta.trend.EMAIndicator(close=data['Close'], window=48)\n",
    "        \n",
    "    # Calcular las EMAs 13 y 48 \n",
    "    data['Ema 13'] = ema_13_indicator.ema_indicator()\n",
    "    data['Ema 48'] = ema_48_indicator.ema_indicator()\n",
    "    # Inicializar el indicador EMA con una ventana de 200 períodos\n",
    "    ema_indicator = ta.trend.EMAIndicator(close=data['Close'], window=200)\n",
    "    # Calcular la EMA 200\n",
    "\n",
    "    data['Ema 200'] = ema_indicator.ema_indicator()\n",
    "    data[\"Long\"] = False\n",
    "    data[\"Short\"] = False\n",
    "\n",
    "    for i in range(len(data[\"Close\"])):\n",
    "\n",
    "        if i + 10 < len(data[\"Close\"]):\n",
    "            if data[\"Close\"][i] > data[\"Close\"][i + 10]:\n",
    "                data.loc[i, \"Long\"] = True\n",
    "            elif data[\"Close\"][i] < data[\"Close\"][i + 10]:\n",
    "                data.loc[i, \"Short\"] = True\n",
    "    data=data\n",
    "    return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_data(url).iloc[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_long(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Long']\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo de regresión logística\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Imprimir el informe de clasificación\n",
    "    #print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5177104722792608\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_short(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Short']\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo de regresión logística\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Imprimir el informe de clasificación\n",
    "    #print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5133470225872689\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_short(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine_long(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Long']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el clasificador SVM\n",
    "    clf = svm.SVC(kernel='linear')  # Puedes cambiar el kernel según tus necesidades (lineal, polinomial, RBF, etc.)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5236139630390144\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine_short(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Short']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el clasificador SVM\n",
    "    clf = svm.SVC(kernel='linear')  # Puedes cambiar el kernel según tus necesidades (lineal, polinomial, RBF, etc.)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5207905544147844\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine_short(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_long(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Long']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convertir los datos a un formato específico para XGBoost (DMatrix)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Definir los parámetros del modelo\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',  # Problema de clasificación binaria\n",
    "        'eval_metric': 'logloss',  # Métrica de evaluación\n",
    "        'eta': 0.1,  # Tasa de aprendizaje\n",
    "        'max_depth': 6,  # Profundidad máxima del árbol\n",
    "        'subsample': 0.8,  # Proporción de muestras utilizadas para entrenar cada árbol\n",
    "        'colsample_bytree': 0.8  # Proporción de características utilizadas para entrenar cada árbol\n",
    "    }\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    num_round = 100  # Número de iteraciones de entrenamiento (número de árboles)\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = [1 if pred > 0.5 else 0 for pred in y_pred_proba]  # Convertir probabilidades en etiquetas binarias\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6763347022587269\n"
     ]
    }
   ],
   "source": [
    "XGBoost_long(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_short(data):\n",
    "\n",
    "    X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "    y = data['Short']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convertir los datos a un formato específico para XGBoost (DMatrix)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Definir los parámetros del modelo\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',  # Problema de clasificación binaria\n",
    "        'eval_metric': 'logloss',  # Métrica de evaluación\n",
    "        'eta': 0.1,  # Tasa de aprendizaje\n",
    "        'max_depth': 6,  # Profundidad máxima del árbol\n",
    "        'subsample': 0.8,  # Proporción de muestras utilizadas para entrenar cada árbol\n",
    "        'colsample_bytree': 0.8  # Proporción de características utilizadas para entrenar cada árbol\n",
    "    }\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    num_round = 100  # Número de iteraciones de entrenamiento (número de árboles)\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = [1 if pred > 0.5 else 0 for pred in y_pred_proba]  # Convertir probabilidades en etiquetas binarias\n",
    "\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6817248459958932\n"
     ]
    }
   ],
   "source": [
    "XGBoost_short(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Closet-1</th>\n",
       "      <th>Closet-2</th>\n",
       "      <th>RSI</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>Ema 13</th>\n",
       "      <th>Ema 48</th>\n",
       "      <th>Ema 200</th>\n",
       "      <th>Long</th>\n",
       "      <th>Short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>126.189903</td>\n",
       "      <td>126.087997</td>\n",
       "      <td>125.817901</td>\n",
       "      <td>41.816133</td>\n",
       "      <td>-0.090918</td>\n",
       "      <td>-0.041450</td>\n",
       "      <td>126.404885</td>\n",
       "      <td>126.524379</td>\n",
       "      <td>126.837699</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>126.087997</td>\n",
       "      <td>125.817901</td>\n",
       "      <td>125.869903</td>\n",
       "      <td>39.595324</td>\n",
       "      <td>-0.108543</td>\n",
       "      <td>-0.054869</td>\n",
       "      <td>126.359615</td>\n",
       "      <td>126.506567</td>\n",
       "      <td>126.830240</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>125.817901</td>\n",
       "      <td>125.869903</td>\n",
       "      <td>126.660003</td>\n",
       "      <td>34.383179</td>\n",
       "      <td>-0.142660</td>\n",
       "      <td>-0.072427</td>\n",
       "      <td>126.282227</td>\n",
       "      <td>126.478458</td>\n",
       "      <td>126.820167</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>125.869903</td>\n",
       "      <td>126.660003</td>\n",
       "      <td>126.480003</td>\n",
       "      <td>36.126510</td>\n",
       "      <td>-0.163617</td>\n",
       "      <td>-0.090665</td>\n",
       "      <td>126.223324</td>\n",
       "      <td>126.453619</td>\n",
       "      <td>126.810711</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>126.660003</td>\n",
       "      <td>126.480003</td>\n",
       "      <td>126.925003</td>\n",
       "      <td>55.480222</td>\n",
       "      <td>-0.115143</td>\n",
       "      <td>-0.095561</td>\n",
       "      <td>126.285707</td>\n",
       "      <td>126.462043</td>\n",
       "      <td>126.809212</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19674</th>\n",
       "      <td>192.500000</td>\n",
       "      <td>192.510299</td>\n",
       "      <td>192.369995</td>\n",
       "      <td>54.044886</td>\n",
       "      <td>0.097370</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>192.505425</td>\n",
       "      <td>192.437451</td>\n",
       "      <td>192.983700</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19675</th>\n",
       "      <td>192.510299</td>\n",
       "      <td>192.369995</td>\n",
       "      <td>192.520004</td>\n",
       "      <td>54.486588</td>\n",
       "      <td>0.089797</td>\n",
       "      <td>0.082760</td>\n",
       "      <td>192.506122</td>\n",
       "      <td>192.440425</td>\n",
       "      <td>192.978990</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19676</th>\n",
       "      <td>192.369995</td>\n",
       "      <td>192.520004</td>\n",
       "      <td>192.529998</td>\n",
       "      <td>47.752875</td>\n",
       "      <td>0.071648</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>192.486675</td>\n",
       "      <td>192.437550</td>\n",
       "      <td>192.972930</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19677</th>\n",
       "      <td>192.520004</td>\n",
       "      <td>192.529998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.261364</td>\n",
       "      <td>0.068578</td>\n",
       "      <td>0.078145</td>\n",
       "      <td>192.491436</td>\n",
       "      <td>192.440915</td>\n",
       "      <td>192.968424</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19678</th>\n",
       "      <td>192.529998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.666539</td>\n",
       "      <td>0.066189</td>\n",
       "      <td>0.075754</td>\n",
       "      <td>192.496945</td>\n",
       "      <td>192.444551</td>\n",
       "      <td>192.964061</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19479 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close    Closet-1    Closet-2        RSI      macd  macd_signal  \\\n",
       "200    126.189903  126.087997  125.817901  41.816133 -0.090918    -0.041450   \n",
       "201    126.087997  125.817901  125.869903  39.595324 -0.108543    -0.054869   \n",
       "202    125.817901  125.869903  126.660003  34.383179 -0.142660    -0.072427   \n",
       "203    125.869903  126.660003  126.480003  36.126510 -0.163617    -0.090665   \n",
       "204    126.660003  126.480003  126.925003  55.480222 -0.115143    -0.095561   \n",
       "...           ...         ...         ...        ...       ...          ...   \n",
       "19674  192.500000  192.510299  192.369995  54.044886  0.097370     0.081000   \n",
       "19675  192.510299  192.369995  192.520004  54.486588  0.089797     0.082760   \n",
       "19676  192.369995  192.520004  192.529998  47.752875  0.071648     0.080537   \n",
       "19677  192.520004  192.529998         NaN  54.261364  0.068578     0.078145   \n",
       "19678  192.529998         NaN         NaN  54.666539  0.066189     0.075754   \n",
       "\n",
       "           Ema 13      Ema 48     Ema 200   Long  Short  \n",
       "200    126.404885  126.524379  126.837699  False   True  \n",
       "201    126.359615  126.506567  126.830240  False   True  \n",
       "202    126.282227  126.478458  126.820167  False   True  \n",
       "203    126.223324  126.453619  126.810711  False   True  \n",
       "204    126.285707  126.462043  126.809212   True  False  \n",
       "...           ...         ...         ...    ...    ...  \n",
       "19674  192.505425  192.437451  192.983700  False  False  \n",
       "19675  192.506122  192.440425  192.978990  False  False  \n",
       "19676  192.486675  192.437550  192.972930  False  False  \n",
       "19677  192.491436  192.440915  192.968424  False  False  \n",
       "19678  192.496945  192.444551  192.964061  False  False  \n",
       "\n",
       "[19479 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-10 22:06:31,205] A new study created in memory with name: no-name-7047313f-8ab3-4135-99af-3929d84334d7\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:07:55,528] Trial 0 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.013440887449621339, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:07:57,086] Trial 1 finished with value: 0.5303655109878669 and parameters: {'model': 'xgboost', 'eta': 0.014714376654171704, 'max_depth': 5, 'subsample': 0.8099555173437751, 'colsample_bytree': 0.6810421495849917}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:08:00,335] Trial 2 finished with value: 0.4966368982505345 and parameters: {'model': 'xgboost', 'eta': 0.0513956771520127, 'max_depth': 10, 'subsample': 0.890154443025076, 'colsample_bytree': 0.8771131872562989}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:08:00,778] Trial 3 finished with value: 0.520971458169137 and parameters: {'model': 'logistic_regression', 'C': 0.22536644546384418, 'max_iter': 517}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:09:27,148] Trial 4 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.21299237679777588, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:09:27,572] Trial 5 finished with value: 0.5209201234668782 and parameters: {'model': 'logistic_regression', 'C': 3.8989536193853316, 'max_iter': 819}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:09:35,918] Trial 6 finished with value: 0.5047999857659875 and parameters: {'model': 'xgboost', 'eta': 0.01835268339879757, 'max_depth': 9, 'subsample': 0.8796270113212595, 'colsample_bytree': 0.8121237382201899}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:09:37,197] Trial 7 finished with value: 0.5302113750846793 and parameters: {'model': 'xgboost', 'eta': 0.01705862376173614, 'max_depth': 5, 'subsample': 0.7335806940067148, 'colsample_bytree': 0.9896131643064552}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:09:37,530] Trial 8 finished with value: 0.5210228060510369 and parameters: {'model': 'logistic_regression', 'C': 0.07328950295661124, 'max_iter': 517}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:09:37,858] Trial 9 finished with value: 0.5209201234668782 and parameters: {'model': 'logistic_regression', 'C': 2.8680558132973846, 'max_iter': 306}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:12:28,546] Trial 10 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.010180919440997995, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:14:33,023] Trial 11 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.011469041209153917, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:19:31,106] Trial 12 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.07852191100692851, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:22:18,555] Trial 13 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.7340231461926933, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:22:54,003] Trial 14 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.04579634990987102, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:24:25,470] Trial 15 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.62565751164952, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:25:01,778] Trial 16 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.030437477638327503, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:41:46,459] Trial 17 finished with value: 0.5238460565195731 and parameters: {'model': 'svm', 'C': 9.59219420291086, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:43:42,024] Trial 18 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.5653621678863382, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:44:57,371] Trial 19 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.18159477636243096, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:45:28,800] Trial 20 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.026203558377104454, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:46:32,083] Trial 21 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.010613343510786471, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:47:36,063] Trial 22 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.017417020768737405, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:48:42,770] Trial 23 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.1279178902694116, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:49:46,885] Trial 24 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.023465033318950716, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:50:52,306] Trial 25 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.05355724574819354, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:55:45,462] Trial 26 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 1.4089913175860729, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:56:16,421] Trial 27 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.01563631127557303, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:56:16,958] Trial 28 finished with value: 0.5263617732416381 and parameters: {'model': 'xgboost', 'eta': 0.09546558743518851, 'max_depth': 3, 'subsample': 0.6132059658101214, 'colsample_bytree': 0.6180898002741522}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:56:17,149] Trial 29 finished with value: 0.520971458169137 and parameters: {'model': 'logistic_regression', 'C': 0.10782987553806923, 'max_iter': 100}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:56:18,195] Trial 30 finished with value: 0.5042865992044769 and parameters: {'model': 'xgboost', 'eta': 0.038646456074494545, 'max_depth': 8, 'subsample': 0.9976110864677569, 'colsample_bytree': 0.9704673963690104}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:56:48,316] Trial 31 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.011347585398363277, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:57:20,465] Trial 32 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.03364499419038327, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:57:51,301] Trial 33 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.013810657053911352, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:58:54,176] Trial 34 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.010108591436485168, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:58:54,364] Trial 35 finished with value: 0.521176796978172 and parameters: {'model': 'logistic_regression', 'C': 0.021739881379222305, 'max_iter': 971}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:58:55,258] Trial 36 finished with value: 0.5243594826200073 and parameters: {'model': 'xgboost', 'eta': 0.010414944315676805, 'max_depth': 7, 'subsample': 0.6005643126189668, 'colsample_bytree': 0.7391551723246421}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:59:26,315] Trial 37 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.03799242818291112, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 22:59:26,518] Trial 38 finished with value: 0.5209201234668782 and parameters: {'model': 'logistic_regression', 'C': 0.27986664924330884, 'max_iter': 765}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 22:59:27,012] Trial 39 finished with value: 0.5203033162613048 and parameters: {'model': 'xgboost', 'eta': 0.09890997534509233, 'max_depth': 3, 'subsample': 0.9975801180998183, 'colsample_bytree': 0.8741367048072257}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:00:32,023] Trial 40 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.0179288273202437, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:03:48,333] Trial 41 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.382803527086012, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:06:04,201] Trial 42 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.08603812777333097, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:08:24,735] Trial 43 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.17541487537303538, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:10:24,272] Trial 44 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.06109633248323092, 'kernel': 'linear'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:11:21,419] Trial 45 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.3564016944788656, 'kernel': 'poly'}. Best is trial 0 with value: 0.5337542998579234.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:12:30,442] Trial 46 finished with value: 0.5339083039646997 and parameters: {'model': 'svm', 'C': 1.20072082689654, 'kernel': 'rbf'}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:12:30,639] Trial 47 finished with value: 0.5209201234668782 and parameters: {'model': 'logistic_regression', 'C': 1.4161020102164252, 'max_iter': 103}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:13:38,853] Trial 48 finished with value: 0.5334977054244767 and parameters: {'model': 'svm', 'C': 3.083325369749662, 'kernel': 'rbf'}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:14:47,971] Trial 49 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 1.0297070725916218, 'kernel': 'rbf'}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:15:57,084] Trial 50 finished with value: 0.5313926399611991 and parameters: {'model': 'svm', 'C': 5.827415942510197, 'kernel': 'rbf'}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:17:04,384] Trial 51 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.513735563962263, 'kernel': 'rbf'}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:17:41,936] Trial 52 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.20384835083594013, 'kernel': 'poly'}. Best is trial 46 with value: 0.5339083039646997.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:23:50,151] Trial 53 finished with value: 0.5346270293352452 and parameters: {'model': 'svm', 'C': 2.0559237163674142, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:30:30,283] Trial 54 finished with value: 0.5331384152272302 and parameters: {'model': 'svm', 'C': 2.3627636232542155, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:31:37,009] Trial 55 finished with value: 0.5333436354194949 and parameters: {'model': 'svm', 'C': 1.7736524912304008, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:33:55,073] Trial 56 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.9383664393250877, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:35:01,959] Trial 57 finished with value: 0.5331895654145129 and parameters: {'model': 'svm', 'C': 2.304549233046047, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-10 23:35:02,599] Trial 58 finished with value: 0.5260016527269996 and parameters: {'model': 'xgboost', 'eta': 0.026268657827532543, 'max_depth': 5, 'subsample': 0.6971371797450205, 'colsample_bytree': 0.7562174111944173}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:45:48,371] Trial 59 finished with value: 0.5235893039304326 and parameters: {'model': 'svm', 'C': 5.03810566191774, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:51:23,864] Trial 60 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 8.527390114571418, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:52:53,430] Trial 61 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.012829610663307166, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:54:29,626] Trial 62 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.020919414287176894, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:56:42,753] Trial 63 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.14771523446213275, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:57:46,971] Trial 64 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.04298817586054866, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:57:47,146] Trial 65 finished with value: 0.5211768101578131 and parameters: {'model': 'logistic_regression', 'C': 0.026980004766293742, 'max_iter': 338}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:58:52,061] Trial 66 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.07117564383067504, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-10 23:59:34,045] Trial 67 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.25414787248004667, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:01:07,280] Trial 68 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.015743711982481437, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-11 00:01:08,321] Trial 69 finished with value: 0.5170173285921771 and parameters: {'model': 'xgboost', 'eta': 0.05929338946214101, 'max_depth': 10, 'subsample': 0.899262754934282, 'colsample_bytree': 0.6083885294922305}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:02:14,703] Trial 70 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.5123806240136104, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:03:27,644] Trial 71 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.7830099537722225, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:04:44,299] Trial 72 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 1.2331526177024965, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:06:12,009] Trial 73 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.6777739651381625, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:10:09,710] Trial 74 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 1.7595862779063025, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:11:27,155] Trial 75 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.012959589763759355, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:11:27,369] Trial 76 finished with value: 0.520868736046055 and parameters: {'model': 'logistic_regression', 'C': 0.010033021021943724, 'max_iter': 700}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 00:11:59,727] Trial 77 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.019107956378893422, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:42:10,027] Trial 78 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.09717343877957181, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:45:27,309] Trial 79 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.45046276543958924, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-11 16:45:28,165] Trial 80 finished with value: 0.5066989216417616 and parameters: {'model': 'xgboost', 'eta': 0.02844848829821274, 'max_depth': 7, 'subsample': 0.7017917740117433, 'colsample_bytree': 0.8965221192065502}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:45:57,111] Trial 81 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.04967636626593381, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:46:24,955] Trial 82 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.03127785747965503, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:46:56,570] Trial 83 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.1131585591490084, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:47:26,434] Trial 84 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.014367971170298472, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:48:25,456] Trial 85 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.02437554546256292, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:48:57,408] Trial 86 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.14529957432931245, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:50:14,562] Trial 87 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.0119480557265778, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:51:32,387] Trial 88 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.07873525070544742, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:51:32,611] Trial 89 finished with value: 0.5211254622759132 and parameters: {'model': 'logistic_regression', 'C': 0.04126327362458228, 'max_iter': 373}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:53:00,938] Trial 90 finished with value: 0.5337029651556648 and parameters: {'model': 'svm', 'C': 0.8851550116629343, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:54:15,042] Trial 91 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.6770330047424562, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:55:45,060] Trial 92 finished with value: 0.5339083039646997 and parameters: {'model': 'svm', 'C': 1.1255087578767027, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:57:08,607] Trial 93 finished with value: 0.533292287537595 and parameters: {'model': 'svm', 'C': 1.3638820407017702, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 16:59:01,183] Trial 94 finished with value: 0.5332923007172361 and parameters: {'model': 'svm', 'C': 1.747244672293377, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:02:56,542] Trial 95 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 2.294027958744402, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:04:20,773] Trial 96 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 0.017369727698348082, 'kernel': 'linear'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:05:26,304] Trial 97 finished with value: 0.5332924061543652 and parameters: {'model': 'svm', 'C': 3.596282830752117, 'kernel': 'rbf'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:06,457] Trial 98 finished with value: 0.5337542998579234 and parameters: {'model': 'svm', 'C': 1.2508930891424428, 'kernel': 'poly'}. Best is trial 53 with value: 0.5346270293352452.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\1195554936.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-11 17:08:06,966] Trial 99 finished with value: 0.5056205502236586 and parameters: {'model': 'xgboost', 'eta': 0.06368353989493732, 'max_depth': 4, 'subsample': 0.7833115331707007, 'colsample_bytree': 0.6856328623858824}. Best is trial 53 with value: 0.5346270293352452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'model': 'svm', 'C': 2.0559237163674142, 'kernel': 'linear'}\n",
      "Best Score: 0.5346270293352452\n",
      "Optimal model: svm\n",
      "Optimal C: 2.0559237163674142\n",
      "Optimal kernel: linear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizing using \"Long\" as the target variable\n",
    "X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "y = data['Long']\n",
    "\n",
    "# Optimizing Logistic Regression\n",
    "def optimize_logistic_regression(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000) # Maximum number of iterations\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=max_iter)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing SVM\n",
    "def optimize_svm(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly']) # Kernel type\n",
    "    \n",
    "    model = svm.SVC(C=C, kernel=kernel)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing XGBoost\n",
    "def optimize_xgboost(trial):\n",
    "    eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10) # Maximum depth of the tree\n",
    "    subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
    "    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
    "    \n",
    "    model = xgb.XGBClassifier(eta=eta, max_depth=max_depth, subsample=subsample, colsample_bytree=colsample_bytree)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Ojective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['logistic_regression', 'svm', 'xgboost'])\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        return optimize_logistic_regression(trial)\n",
    "    elif model_name == 'svm':\n",
    "        return optimize_svm(trial)\n",
    "    elif model_name == 'xgboost':\n",
    "        return optimize_xgboost(trial)\n",
    "\n",
    "# Optimize hyperparameters using Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100) # We decided to do 100 trials becasue we noticed that the the study convverges\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Print optimal hyperparameters\n",
    "for param, value in best_params.items():\n",
    "    print(f\"Optimal {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 17:08:07,085] A new study created in memory with name: no-name-1b40a541-e042-41fc-aa40-ba76bb108a42\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:47,580] Trial 0 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.19877115453280686, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:47,749] Trial 1 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 0.7859636119086642, 'max_iter': 347}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-11 17:08:49,592] Trial 2 finished with value: 0.5276450748999006 and parameters: {'model': 'xgboost', 'eta': 0.017347918166329076, 'max_depth': 8, 'subsample': 0.6809445399654794, 'colsample_bytree': 0.614044431902734}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:49,766] Trial 3 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 0.06937861882673671, 'max_iter': 433}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:49,939] Trial 4 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 0.1848735178423306, 'max_iter': 935}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:50,101] Trial 5 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 3.037695542856954, 'max_iter': 112}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:50,277] Trial 6 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 6.318397785937001, 'max_iter': 291}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-11 17:08:51,292] Trial 7 finished with value: 0.5141431651698988 and parameters: {'model': 'xgboost', 'eta': 0.06564344851210697, 'max_depth': 4, 'subsample': 0.9787650658614274, 'colsample_bytree': 0.78430824118249}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:51,451] Trial 8 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 0.3366374284306476, 'max_iter': 574}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:08:51,611] Trial 9 finished with value: 0.5166590664069399 and parameters: {'model': 'logistic_regression', 'C': 3.350327173105409, 'max_iter': 250}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:09:20,132] Trial 10 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.013484105817928861, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:09:48,606] Trial 11 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.010782942144520006, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:10:17,367] Trial 12 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.014121912832586524, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:11:20,678] Trial 13 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.05329616586111576, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:11:51,770] Trial 14 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.04949694946517956, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:14:52,625] Trial 15 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.8943565233605743, 'kernel': 'linear'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:15:22,756] Trial 16 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.02272102491315011, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:15:59,130] Trial 17 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.1565887798919867, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:17:03,845] Trial 18 finished with value: 0.53052005545993 and parameters: {'model': 'svm', 'C': 0.6211303649277424, 'kernel': 'rbf'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:29: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
      "[I 2024-03-11 17:17:06,600] Trial 19 finished with value: 0.5111653965885816 and parameters: {'model': 'xgboost', 'eta': 0.010152563872035687, 'max_depth': 10, 'subsample': 0.6048773974992466, 'colsample_bytree': 0.984822313119099}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:19:25,024] Trial 20 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.028166373713140878, 'kernel': 'linear'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:19:58,749] Trial 21 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.01975693664660591, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:20:36,203] Trial 22 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.091920567451986, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[I 2024-03-11 17:21:26,317] Trial 23 finished with value: 0.5305713901621887 and parameters: {'model': 'svm', 'C': 0.030126886666158204, 'kernel': 'poly'}. Best is trial 0 with value: 0.5305713901621887.\n",
      "C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
      "[W 2024-03-11 17:21:35,681] Trial 24 failed with parameters: {'model': 'svm', 'C': 0.0140605313004341, 'kernel': 'poly'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py\", line 44, in objective\n",
      "    return optimize_svm(trial)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_19116\\3604242193.py\", line 21, in optimize_svm\n",
      "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 714, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 425, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 250, in fit\n",
      "    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n",
      "  File \"c:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 329, in _dense_fit\n",
      "    ) = libsvm.fit(\n",
      "        ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-11 17:21:35,884] Trial 24 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Optimize hyperparameters using Optuna\u001b[39;00m\n\u001b[0;32m     49\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# We decided to do 100 trials becasue we noticed that the the study converges\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters and corresponding score\u001b[39;00m\n\u001b[0;32m     53\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 44\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize_logistic_regression(trial)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize_xgboost(trial)\n",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m, in \u001b[0;36moptimize_svm\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m kernel \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# Kernel type\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(C\u001b[38;5;241m=\u001b[39mC, kernel\u001b[38;5;241m=\u001b[39mkernel)\n\u001b[1;32m---> 21\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 5-fold cross-validation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:714\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    712\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 714\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 425\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:890\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 890\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\erick\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimizing using \"Short\" as the target variable\n",
    "X = data[['RSI', 'Ema 13', 'Ema 200']]\n",
    "y = data['Short']\n",
    "\n",
    "# Optimizing Logistic Regression\n",
    "def optimize_logistic_regression(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000) # Maximum number of iterations\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=max_iter)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing SVM\n",
    "def optimize_svm(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.01, 10) # Regularization parameter\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly']) # Kernel type\n",
    "    \n",
    "    model = svm.SVC(C=C, kernel=kernel)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optimizing XGBoost\n",
    "def optimize_xgboost(trial):\n",
    "    eta = trial.suggest_loguniform('eta', 0.01, 0.1) # Learning rate\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10) # Maximum depth of the tree\n",
    "    subsample = trial.suggest_uniform('subsample', 0.6, 1.0) # Subsample ratio of the training instances\n",
    "    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0) # Subsample ratio of columns when constructing each tree\n",
    "    \n",
    "    model = xgb.XGBClassifier(eta=eta, max_depth=max_depth, subsample=subsample, colsample_bytree=colsample_bytree)\n",
    "    scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Ojective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['logistic_regression', 'svm', 'xgboost'])\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        return optimize_logistic_regression(trial)\n",
    "    elif model_name == 'svm':\n",
    "        return optimize_svm(trial)\n",
    "    elif model_name == 'xgboost':\n",
    "        return optimize_xgboost(trial)\n",
    "\n",
    "# Optimize hyperparameters using Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100) # We decided to do 100 trials becasue we noticed that the the study converges\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Print optimal hyperparameters\n",
    "for param, value in best_params.items():\n",
    "    print(f\"Optimal {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_logistic_regression(data, model, target, take_profit, stop_loss, initial_capital, margin_rate):\n",
    "    # Crear una copia de los datos\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Crear una nueva columna con las predicciones del modelo\n",
    "    data['Prediction'] = model.predict(data[['RSI', 'Ema 13', 'Ema 200']])\n",
    "    \n",
    "    # Calcular el rendimiento diario\n",
    "    data['Return'] = data['Closet-1'] / data['Close'] - 1\n",
    "    \n",
    "    # Calcular el rendimiento diario de la estrategia\n",
    "    data['Strategy Return'] = data['Return'] * data['Prediction']\n",
    "    \n",
    "    # Calcular el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'] = (data['Strategy Return'] + 1).cumprod()\n",
    "    \n",
    "    # Inicializar el capital\n",
    "    capital = initial_capital\n",
    "    \n",
    "    # Inicializar el estado de la operación\n",
    "    in_trade = False\n",
    "    \n",
    "    # Inicializar el precio de entrada\n",
    "    entry_price = 0\n",
    "    \n",
    "    # Recorrer los datos\n",
    "    for i in range(len(data)):\n",
    "        # Si estamos en una operación\n",
    "        if in_trade:\n",
    "            # Si alcanzamos el take profit o el stop loss\n",
    "            if data['Close'].iloc[i] >= entry_price * (1 + take_profit) or data['Close'].iloc[i] <= entry_price * (1 - stop_loss):\n",
    "                # Salir de la operación\n",
    "                in_trade = False\n",
    "                # Agregar los fondos de margen utilizados al capital\n",
    "                capital += (entry_price - data['Close'].iloc[i]) * margin_rate\n",
    "        # Si no estamos en una operación y el modelo predice una entrada\n",
    "        elif data['Prediction'].iloc[i] == 1:\n",
    "            # Entrar en la operación\n",
    "            in_trade = True\n",
    "            entry_price = data['Close'].iloc[i]\n",
    "            # Use only 10% of the capital for each trade\n",
    "            trade_amount = capital * 0.00001\n",
    "            capital -= trade_amount  # Descontar el precio de entrada del capital\n",
    "    \n",
    "    # Graficar el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'].plot(figsize=(10, 6))\n",
    "    plt.title(f'Cumulative Strategy Return ({target})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(data[target], data['Prediction'])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Imprimir el informe de clasificación\n",
    "    print(classification_report(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir la matriz de confusión\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir el capital final\n",
    "    print(\"Final capital:\", capital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo de regresión logística para \"Long\"\n",
    "model = LogisticRegression(C=0.01, max_iter=100)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Long'])\n",
    "backtest_logistic_regression(data, model, 'Long', 0.05, 0.02, 1000000, 0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo de regresión logística para \"Short\"\n",
    "model = LogisticRegression(C=0.01, max_iter=100)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Short'])\n",
    "backtest_logistic_regression(data, model, 'Short', 0.05, 0.02, 1000000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_svm(data, model, target, take_profit, stop_loss, initial_capital, margin_rate):\n",
    "    # Crear una copia de los datos\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Crear una nueva columna con las predicciones del modelo\n",
    "    data['Prediction'] = model.predict(data[['RSI', 'Ema 13', 'Ema 200']])\n",
    "    \n",
    "    # Calcular el rendimiento diario\n",
    "    data['Return'] = data['Closet-1'] / data['Close'] - 1\n",
    "    \n",
    "    # Calcular el rendimiento diario de la estrategia\n",
    "    data['Strategy Return'] = data['Return'] * data['Prediction']\n",
    "    \n",
    "    # Calcular el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'] = (data['Strategy Return'] + 1).cumprod()\n",
    "    \n",
    "    # Inicializar el capital\n",
    "    capital = initial_capital\n",
    "    \n",
    "    # Inicializar el estado de la operación\n",
    "    in_trade = False\n",
    "    \n",
    "    # Inicializar el precio de entrada\n",
    "    entry_price = 0\n",
    "    \n",
    "    # Recorrer los datos\n",
    "    for i in range(len(data)):\n",
    "        # Si estamos en una operación\n",
    "        if in_trade:\n",
    "            # Si alcanzamos el take profit o el stop loss\n",
    "            if data['Close'].iloc[i] >= entry_price * (1 + take_profit) or data['Close'].iloc[i] <= entry_price * (1 - stop_loss):\n",
    "                # Salir de la operación\n",
    "                in_trade = False\n",
    "                # Agregar los fondos de margen utilizados al capital\n",
    "                capital += (entry_price - data['Close'].iloc[i]) * margin_rate\n",
    "        # Si no estamos en una operación y el modelo predice una entrada\n",
    "        elif data['Prediction'].iloc[i] == 1:\n",
    "            # Entrar en la operación\n",
    "            in_trade = True\n",
    "            entry_price = data['Close'].iloc[i]\n",
    "            # Use only 10% of the capital for each trade\n",
    "            trade_amount = capital * 0.00001\n",
    "            capital -= trade_amount  # Descontar el precio de entrada del capital\n",
    "\n",
    "    # Graficar el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'].plot(figsize=(10, 6))\n",
    "    plt.title(f'Cumulative Strategy Return ({target})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(data[target], data['Prediction'])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Imprimir el informe de clasificación\n",
    "    print(classification_report(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir la matriz de confusión\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir el capital final\n",
    "    print(\"Final capital:\", capital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo SVM para \"Long\"\n",
    "model = svm.SVC(C=0.01, kernel='rbf')\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Long'])\n",
    "backtest_svm(data, model, 'Long', 0.05, 0.02, 1000000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo SVM para \"Short\"\n",
    "model = svm.SVC(C=0.01, kernel='rbf')\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Short'])\n",
    "backtest_svm(data, model, 'Short', 0.05, 0.02, 1000000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_xgboost(data, model, target, take_profit, stop_loss, initial_capital, margin_rate):\n",
    "    # Crear una copia de los datos\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Crear una nueva columna con las predicciones del modelo\n",
    "    data['Prediction'] = model.predict(data[['RSI', 'Ema 13', 'Ema 200']])\n",
    "    \n",
    "    # Calcular el rendimiento diario\n",
    "    data['Return'] = data['Closet-1'] / data['Close'] - 1\n",
    "    \n",
    "    # Calcular el rendimiento diario de la estrategia\n",
    "    data['Strategy Return'] = data['Return'] * data['Prediction']\n",
    "    \n",
    "    # Calcular el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'] = (data['Strategy Return'] + 1).cumprod()\n",
    "    \n",
    "    # Inicializar el capital\n",
    "    capital = initial_capital\n",
    "    \n",
    "    # Inicializar el estado de la operación\n",
    "    in_trade = False\n",
    "    \n",
    "    # Inicializar el precio de entrada\n",
    "    entry_price = 0\n",
    "    \n",
    "    # Recorrer los datos\n",
    "    for i in range(len(data)):\n",
    "        # Si estamos en una operación\n",
    "        if in_trade:\n",
    "            # Si alcanzamos el take profit o el stop loss\n",
    "            if data['Close'].iloc[i] >= entry_price * (1 + take_profit) or data['Close'].iloc[i] <= entry_price * (1 - stop_loss):\n",
    "                # Salir de la operación\n",
    "                in_trade = False\n",
    "                 # Agregar los fondos de margen utilizados al capital\n",
    "                capital += (entry_price - data['Close'].iloc[i]) * margin_rate\n",
    "        # Si no estamos en una operación y el modelo predice una entrada\n",
    "        elif data['Prediction'].iloc[i] == 1:\n",
    "            # Entrar en la operación\n",
    "            in_trade = True\n",
    "            entry_price = data['Close'].iloc[i]\n",
    "            # Use only 10% of the capital for each trade\n",
    "            trade_amount = capital * 0.000010\n",
    "            capital -= trade_amount  # Descontar el precio de entrada del capital\n",
    "    \n",
    "    # Graficar el rendimiento acumulado de la estrategia\n",
    "    data['Cumulative Strategy Return'].plot(figsize=(10, 6))\n",
    "    plt.title(f'Cumulative Strategy Return ({target})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(data[target], data['Prediction'])\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Imprimir el informe de clasificación\n",
    "    print(classification_report(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir la matriz de confusión\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(data[target], data['Prediction']))\n",
    "    \n",
    "    # Imprimir el capital final\n",
    "    print(\"Final capital:\", capital)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo XGBoost para \"Long\"\n",
    "model = xgb.XGBClassifier(eta=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Long'])\n",
    "backtest_xgboost(data, model, 'Long', 0.05, 0.02, 1000000, 0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar la estrategia con el modelo XGBoost para \"Short\"\n",
    "model = xgb.XGBClassifier(eta=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8)\n",
    "model.fit(data[['RSI', 'Ema 13', 'Ema 200']], data['Short'])\n",
    "backtest_xgboost(data, model, 'Short', 0.05, 0.02, 1000000, 0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
